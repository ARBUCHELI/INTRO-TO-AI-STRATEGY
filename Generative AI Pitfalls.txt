# GENERATIVE AI PITFALLS
------------------------

* Introduction
--------------
Our online lives are filled with examples of amazing things Generative AI can do. It’s a powerful tool, and if you’ve used ChatGPT, Dall-E, or another Gen AI tool yourself you’ve probably 
seen this firsthand already!

We can probably agree that not every single problem can be solved using AI. However, Gen AI also has some shortcomings even when tackling tasks it’s specifically designed for and 
generally good at, like language generation. Luckily, many of these challenges are predictable, which means we can avoid or fix them by using Gen AI in the right way, to solve the right 
problems.

Solutions to these Gen AI challenges range from small tweaks like improving our prompts and adjusting model parameters, to larger mindset shifts like factoring in longterm internal goals 
and remembering to take a breath in a crowded, exciting field.

This lesson will walk through six pitfalls to look out for when using generative AI to solve a problem, and show you how to avoid them or be aware of risks:

	. Generalization & Compression

	. Verifiability

	. Scope

	. Vulnerability

	. Future-Proofing

	. Quality Control

Avoiding these issues or having a plan in place to mitigate the risks will help to ensure that generative AI is a useful, efficient, and sustainable tool for you. We’ll focus mostly on 
Large Language Models (LLMs) and Generative Pre-Trained Transformers (GPTs) here, but we’ll talk a little about generative image technologies too.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Generalization & Compression: the Catch-22
--------------------------------------------
Before we jump into some more detailed pitfalls and solutions, it’s helpful to start by understanding the core technology that makes generative AI generative. The key is in compression 
and generalization.

* Compression & Generalization
------------------------------
Compression and generalization help determine how the next word is chosen in a language model. (They also sometimes determine how the next pixel is chosen in an image generator, although 
the most cutting-edge image generators use a slightly different process called stable diffusion.) We’ll walk through the process for a language model.

First, a model is fed lots of data and begins to recognize patterns of which words are often strung together, and with what frequencies. Compression allows the model to not select a word 
that is probable. For example, question is the most likely ending to this phrase because it’s the actual ending to the famous line from William Shakespeare’s Hamlet:

	"To be or not to be: that is the `question`."

Compression allows the model to give this combination a probability of zero, even though it exists and may be a likely answer.

Generalization allows the model to select a different word combination that it has never specifically encountered before in training data:
--------------

	“To be or not to be: that is the `consideration`.”
	“To be or not to be: that is the `uncertainty`.”
	“To be or not to be: that is the `challenge`.”
	“To be or not to be: that is the `crux of it`.”

This is the key aspect of the technology that makes it “generative”: in other words, a combination of words does not need to have existed before to be a possible output of the model. LLMs 
can recognize and recreate the predictable patterns of syntax to actually generate new sentences that make grammatical sense to humans – not just spit out repurposed fragments of their 
training data. This is an amazing feature of LLMs!

* Hallucinations
----------------
The flip side of this technology is that “likely patterns” are not necessarily true facts – untrue results are often called hallucinations. The model has no understanding of whether its 
sentences make logical sense or represent truthful statements.

An LLM could tell us “The lion chases the llama” because it recognizes a pattern of animal nouns and generalization allows it to swap one animal for another. The sentence makes perfect 
grammatical sense, and it’s even easy to imagine that scene. But lions and llamas exist on different continents, so the chance of this sentence being true is pretty much zero. That 
example is easy to catch, but many are not!

* Model Temperature
-------------------
When we talk about increasing the temperature of the model, that means changing probability distributions so that the model will return less and less likely answers. These answers will 
reflect less of what’s already determined by the data, and instead, generate more novel content that could be produced based on the data. A lower temperature will output more 
deterministic answers that are more likely to be true. A higher temperature will create more fiction with more unlikely combinations of words.

Model temperature is often referred to as the model’s “creativity”, but it’s good to remember that that’s really a human-intuitive way of describing changes to the math that powers these 
models.

* RLHF
------
Finally, reproducing likely outcomes includes the good and the bad. In order to keep models from reproducing and amplifying biased information, lots of human time and effort has to go 
into curating model training data to keep the input data toxicity-free. Additionally, reinforcement learning based on human feedback (RLHF) “corrects” model outputs to minimize bias and 
inaccuracies in future runs, and also requires lots of human effort. Both of these processes have to occur before a model is released for public use.

* Instructions
--------------
NOTE: The “lion chases llama” example is pulled from Dr. Kyunghyun Cho’s presentation A slight-less-magical perspective into autoregressive language modeling.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Verifiability
---------------
So, compression and generalization is the real catch-22 of generative AI. In order to generate content that doesn’t already exist, we sign up for some amount of risk that what the model 
produces may be factually inaccurate. There is no escaping the fact that GPTs produce unverified content.

GPTs have generated fake harassment claims and named real people as the perpetrators, made up fake court cases when asked to give a list of precedents, and supported their bogus claims 
with citations of fake newspaper articles from national papers. Verifying a GPT’s output is a crucial step both in preventing misinformation, and in preventing embarrassing situations and 
the need for edits, redactions, or public apologies.

One thing that makes it hard to catch falsehoods is that GPTs don’t communicate information in the same way humans do. They don’t sound uncertain, even when they have no relevant training 
data to work from. GPTs provide information with the same informative, confident, friendly tone no matter what, and they will only say “I don’t know the answer to this” or “I can’t answer 
this” when they’ve been specifically pre-trained to do so. In other words, LLMs don’t know what they don’t know.

This confident tone, along with the way GPTs are marketed (“impressive!”, “powerful!”, “can ace an MIT course in one go!”), contribute to the phenomenon of authority bias. Authority bias 
refers to our greater tendency to believe statements that come from individuals or organizations in power, or those we think of as “experts” in a specific domain. We are naturally less 
likely to disbelieve authority figures and experts – which means it’s doubly important to fact-check GPTs!

So what can we do?

First, always think of yourself as an editor when you use a GPT. GPTs can save us a lot of drafting work, but they do not produce finished content. And while GPT outputs rarely need heavy 
grammatical editing, they definitely need fact-checking.

Second, use the “temperature” parameter to make outputs more deterministic and less generative. “More deterministic” means the GPT will output information that more closely replicates 
what it has “seen” before, rather than generating information that’s more inventive. Lowering the temperature of the model means there will be less variation in results, but also less 
likelihood of unverified material. (For more details, you can refer back to the previous exercise.)

Third, consider what task you’re using a GPT for. For a more creative task, like generating a list of potential content for a newsletter, the more relevant editor’s question might be “is 
this a good idea?”, rather than “is this factually true?” But if you’re using a GPT in a way where the facts matter, like asking it to write a short bio about a famous person, or return a 
list of the top five competitors in a certain industry, then it’s critical to have eagle eyes and be ready to fact-check to ensure a truthful output.

Finally, for all of the reasons we’ve covered, make sure to fact-check any output that will be widely shared.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Scope
-------
GPT stands for Generative Pre-Trained Transformer. When we’re considering the scope of a problem, and if Generative AI can be a solution, we’re probably thinking most about the 
“pre-trained” aspect of a GPT.

GPTs are trained on lots of data. The volume of training data is one of the breakthroughs of the technology in the last few years – it could not have produced high-quality text results 
without the enormous trove of internet data that exists today. However, even with unfathomably large amounts of training data, GPTs don’t know about everything.

One GPT scope issue is that the training data may be outdated, or in other words, limited in temporal scope. As of mid-2023, for example, ChatGPT only knew about events from 2021 or 
earlier. Current events prompts may be impossible for the GPT to answer (although it may not recognize this – one user reported asking for a Superbowl recap and getting a detailed answer 
about a Superbowl that hadn’t happened yet).

Prompts about niche or specialized fields can also pose a challenge. When there are fewer sources to draw from and “remix”, GPTs are more likely to return generated text that is very 
similar to the existing sources. That’s plagiarism, which can lead to disciplinary or legal action and is definitely best avoided! A better solution might be to ask the GPT to return a 
summary of a source and cite the source directly.

On the other hand, very broad or general prompts (like “write me a business plan”) yield weak results as well, because the wide scope of training data may make it harder for the GPT to 
distinguish the relevant or useful patterns in the data. In addition to writing a better prompt, a solution for this issue can be to fine-tune the GPT using training data that’s specific 
to your situation.

Finally, consider the type of problem you’re trying to solve using a GPT. It is certainly possible to use ChatGPT to write wedding vows, or a eulogy, or a speech to address a national 
tragedy. It might even write them “better” than you could. But sometimes, bad writing done by a living, feeling human is better than good writing done by a robot using probability 
distributions.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Vulnerability
---------------
Another pitfall to look out for when using GPTs is security vulnerability. This can take a couple different forms, depending on how you’re implementing a GPT. We’ll talk about data 
security and prompt injections here.

* Data Security
---------------
No matter what, be mindful of what data you put into a GPT. Companies can log every single piece of data their users input. The data is saved on their servers, used for training purposes, 
and is often shared with other companies. Exact privacy and data use policies are still in development and often changing, so it’s important to do your own research and understand how 
your data is stored and where it may be shared to.

Once data is in a GPT, it’s out of your control, so inputting personally identifiable information (PII) into a GPT can make you vulnerable to a serious data breach. OpenAI, for 
had to take ChatGPT offline for about half a day in 2023 to fix a bug that made some users’ credit card information and partial chat history visible to other users.

But what if I want to use my own data to fine-tune a model and improve its performance for a specific task? That’s a great question, and a good use of a GPT! Data can be generalized, 
anonymized, or otherwise transformed to remove sensitive information before sharing it – if your company has policies for sharing data with third-party or external organizations, a GPT 
should fall into that category. Local fine-tuning is also a more secure way to use your own data in a GPT.

Finally, if you’re thinking, what’s the point of keeping data out of a GPT? Don’t they have access to everything already? In fact, they don’t! In general, GPTs are (supposedly) trained on 
publicly available data – like public posts on social media, webpage content, out-of-copyright literature, or news articles from free sources. Anything that requires permission or 
access – like someone accepting a follow request or paying for a subscription – isn’t supposed to be part of the training data. That being said, as of mid-2023 there are multiple 
copyright lawsuits in progress by authors who say ChatGPT has been trained on their copyrighted work without their consent. How these lawsuits play out will hopefully give us more insight 
into ChatGPT’s training process.

* Prompt Injection
------------------
If you use an API to implement a GPT in your personal project (like creating a helper chatbot for your website), there are additional security concerns. One major concern is prompt 
injection, which is when a user hijacks the “setup” of a GPT using specific prompts. This can lead to embarrassing situations for a company, or at worst, data breaches and downed sites. 
We won’t go into technical detail here, but prompt injections can be avoided by following cybersecurity best practices when setting up your own GPT interface.

------------------------------------------------------------------------------------------------------------------------------------------------------------------




































































