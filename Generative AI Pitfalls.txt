# GENERATIVE AI PITFALLS
------------------------

* Introduction
--------------
Our online lives are filled with examples of amazing things Generative AI can do. It’s a powerful tool, and if you’ve used ChatGPT, Dall-E, or another Gen AI tool yourself you’ve probably 
seen this firsthand already!

We can probably agree that not every single problem can be solved using AI. However, Gen AI also has some shortcomings even when tackling tasks it’s specifically designed for and 
generally good at, like language generation. Luckily, many of these challenges are predictable, which means we can avoid or fix them by using Gen AI in the right way, to solve the right 
problems.

Solutions to these Gen AI challenges range from small tweaks like improving our prompts and adjusting model parameters, to larger mindset shifts like factoring in longterm internal goals 
and remembering to take a breath in a crowded, exciting field.

This lesson will walk through six pitfalls to look out for when using generative AI to solve a problem, and show you how to avoid them or be aware of risks:

	. Generalization & Compression

	. Verifiability

	. Scope

	. Vulnerability

	. Future-Proofing

	. Quality Control

Avoiding these issues or having a plan in place to mitigate the risks will help to ensure that generative AI is a useful, efficient, and sustainable tool for you. We’ll focus mostly on 
Large Language Models (LLMs) and Generative Pre-Trained Transformers (GPTs) here, but we’ll talk a little about generative image technologies too.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Generalization & Compression: the Catch-22
--------------------------------------------
Before we jump into some more detailed pitfalls and solutions, it’s helpful to start by understanding the core technology that makes generative AI generative. The key is in compression 
and generalization.

* Compression & Generalization
------------------------------
Compression and generalization help determine how the next word is chosen in a language model. (They also sometimes determine how the next pixel is chosen in an image generator, although 
the most cutting-edge image generators use a slightly different process called stable diffusion.) We’ll walk through the process for a language model.

First, a model is fed lots of data and begins to recognize patterns of which words are often strung together, and with what frequencies. Compression allows the model to not select a word 
that is probable. For example, question is the most likely ending to this phrase because it’s the actual ending to the famous line from William Shakespeare’s Hamlet:

	"To be or not to be: that is the `question`."

Compression allows the model to give this combination a probability of zero, even though it exists and may be a likely answer.

Generalization allows the model to select a different word combination that it has never specifically encountered before in training data:
--------------

	“To be or not to be: that is the `consideration`.”
	“To be or not to be: that is the `uncertainty`.”
	“To be or not to be: that is the `challenge`.”
	“To be or not to be: that is the `crux of it`.”

This is the key aspect of the technology that makes it “generative”: in other words, a combination of words does not need to have existed before to be a possible output of the model. LLMs 
can recognize and recreate the predictable patterns of syntax to actually generate new sentences that make grammatical sense to humans – not just spit out repurposed fragments of their 
training data. This is an amazing feature of LLMs!

* Hallucinations
----------------
The flip side of this technology is that “likely patterns” are not necessarily true facts – untrue results are often called hallucinations. The model has no understanding of whether its 
sentences make logical sense or represent truthful statements.

An LLM could tell us “The lion chases the llama” because it recognizes a pattern of animal nouns and generalization allows it to swap one animal for another. The sentence makes perfect 
grammatical sense, and it’s even easy to imagine that scene. But lions and llamas exist on different continents, so the chance of this sentence being true is pretty much zero. That 
example is easy to catch, but many are not!

* Model Temperature
-------------------
When we talk about increasing the temperature of the model, that means changing probability distributions so that the model will return less and less likely answers. These answers will 
reflect less of what’s already determined by the data, and instead, generate more novel content that could be produced based on the data. A lower temperature will output more 
deterministic answers that are more likely to be true. A higher temperature will create more fiction with more unlikely combinations of words.

Model temperature is often referred to as the model’s “creativity”, but it’s good to remember that that’s really a human-intuitive way of describing changes to the math that powers these 
models.

* RLHF
------
Finally, reproducing likely outcomes includes the good and the bad. In order to keep models from reproducing and amplifying biased information, lots of human time and effort has to go 
into curating model training data to keep the input data toxicity-free. Additionally, reinforcement learning based on human feedback (RLHF) “corrects” model outputs to minimize bias and 
inaccuracies in future runs, and also requires lots of human effort. Both of these processes have to occur before a model is released for public use.

* Instructions
--------------
NOTE: The “lion chases llama” example is pulled from Dr. Kyunghyun Cho’s presentation A slight-less-magical perspective into autoregressive language modeling.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Verifiability
---------------
So, compression and generalization is the real catch-22 of generative AI. In order to generate content that doesn’t already exist, we sign up for some amount of risk that what the model 
produces may be factually inaccurate. There is no escaping the fact that GPTs produce unverified content.

GPTs have generated fake harassment claims and named real people as the perpetrators, made up fake court cases when asked to give a list of precedents, and supported their bogus claims 
with citations of fake newspaper articles from national papers. Verifying a GPT’s output is a crucial step both in preventing misinformation, and in preventing embarrassing situations and 
the need for edits, redactions, or public apologies.

One thing that makes it hard to catch falsehoods is that GPTs don’t communicate information in the same way humans do. They don’t sound uncertain, even when they have no relevant training 
data to work from. GPTs provide information with the same informative, confident, friendly tone no matter what, and they will only say “I don’t know the answer to this” or “I can’t answer 
this” when they’ve been specifically pre-trained to do so. In other words, LLMs don’t know what they don’t know.

This confident tone, along with the way GPTs are marketed (“impressive!”, “powerful!”, “can ace an MIT course in one go!”), contribute to the phenomenon of authority bias. Authority bias 
refers to our greater tendency to believe statements that come from individuals or organizations in power, or those we think of as “experts” in a specific domain. We are naturally less 
likely to disbelieve authority figures and experts – which means it’s doubly important to fact-check GPTs!

So what can we do?

First, always think of yourself as an editor when you use a GPT. GPTs can save us a lot of drafting work, but they do not produce finished content. And while GPT outputs rarely need heavy 
grammatical editing, they definitely need fact-checking.

Second, use the “temperature” parameter to make outputs more deterministic and less generative. “More deterministic” means the GPT will output information that more closely replicates 
what it has “seen” before, rather than generating information that’s more inventive. Lowering the temperature of the model means there will be less variation in results, but also less 
likelihood of unverified material. (For more details, you can refer back to the previous exercise.)

Third, consider what task you’re using a GPT for. For a more creative task, like generating a list of potential content for a newsletter, the more relevant editor’s question might be “is 
this a good idea?”, rather than “is this factually true?” But if you’re using a GPT in a way where the facts matter, like asking it to write a short bio about a famous person, or return a 
list of the top five competitors in a certain industry, then it’s critical to have eagle eyes and be ready to fact-check to ensure a truthful output.

Finally, for all of the reasons we’ve covered, make sure to fact-check any output that will be widely shared.

------------------------------------------------------------------------------------------------------------------------------------------------------------------





































